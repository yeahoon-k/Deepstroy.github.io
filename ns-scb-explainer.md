---
layout: page
title: "Non-Stationary Structural Causal Bandits â€” Explanation"
permalink: /ns-scb-explainer/
---
{% include mathjax.html %}

## TL;DR
We formulate **NS-SCM-MAB** by rolling out a time-indexed **SCM** so that **information propagation** across time is explicit via **transition edges**. **Non-stationarity** is captured as a **reward distribution shift** (conditioned on the intervention history) and changes in temporal information flow. We introduce **POMIS+** with the graphical notions **IB+** and **QIB** to construct **intervention sequences** that remain effective for future rewards, yielding lower **cumulative regret** and higher **optimal-action probability** under shifts.

---

## Problem setting (NS-SCM-MAB)
Let the bandit environment be generated by a time-indexed SCM (i.e., temporal model) $\mathcal{M}_t=\langle \mathbf{U}_t,\mathbf{V}_t,\mathcal{F}_t, P(\mathbf{U}_t)\rangle,\quad t=1,2,\dots$
with reward variable $Y_t\in \mathbf{V}_t$. An **arm** corresponds to an intervention $\operatorname{do}(\mathbf{X}_t{=}\mathbf{x}_t)$ on a **manipulative set** $\mathbf{X}_t \subseteq \mathbf{V}_t$. We use **transition edges** between slices to encode how variables at time $t$ transmit information to variables at $t{+}1$.

A **reward distribution shift** occurs when the conditional reward distribution changes across time given the intervention history $I_{1:t-1}$:
$$
P\!\big(Y_t \mid \operatorname{do}(\mathbf{X}_t{=}\mathbf{x}_t), I_{1:t-1}\big)
\neq
P\!\big(Y_{t'} \mid \operatorname{do}(\mathbf{X}_{t'}{=}\mathbf{x}_{t'}), I_{1:t'-1}\big)
\quad \text{for some } t\neq t'.
$$
Stationary SCM-MAB solutions are **myopic** here because they optimize $Y_t$ in isolation and ignore cross-time information propagation.

---

## Temporal model view
We reason with **temporal models** $\mathcal{M}_t$ that respect cross-time dependencies in the **time-expanded causal diagram**. This clarifies:
- which variables at time $t$ are **parents** (direct or via transitions) of $Y_{t'}$ for $t'>t$,
- which **transition edges** carry useful information toward $Y_{t'}$,
- and how interventions at $t$ influence $Y_{t'}$ beyond a single slice.

This view separates **what changes** (reward distribution shift and effective information flow) from **what remains stable**, enabling **structure-aware** action selection.

---

## POMIS+ (temporal extension of POMIS)
**POMIS** identifies **Possibly-Optimal Manipulative Sets** for a single slice. **POMIS+** extends this idea to time by selecting a sequence
\[
(\mathbf{X}_{t_1}, \mathbf{X}_{t_2}, \dots, \mathbf{X}_{t_k})
\]
such that intervening on these sets at selected steps yields high expected future reward $\mathbb{E}[Y_{t'}]$ for target times $t'\!\ge\! t_k$. Importantly, $\mathbf{X}_{t}$ may include variables **irrelevant to $Y_t$** but **crucial for $Y_{t'}}$ via **transition edges**.

---

## IB+ and QIB (graphical characterization)
To characterize non-myopic interventions on the time-expanded graph:

1. **IB+ (Interventional Border for Subsequent Time Steps).**  
   A graphical frontier that gathers candidates in $\mathbf{V}_t$ whose manipulation can still **alter information propagation** toward $Y_{t'}$.

2. **QIB (Qualified Interventional Border).**  
   A refinement of IB+ that filters out nodes whose manipulation would be redundant given the temporal model (e.g., blocked by **mediators** or **confounders**). QIB retains only candidates that can **change the distribution of ancestors of $Y_{t'}$** that matter.

**POMIS+** is obtained by assembling **QIB-validated** subsets across time that maximize (or approximately maximize) the expected future reward under the **do-operator**.

---

## Why stationary solutions fail
In a **stationary SCM-MAB**, the optimal arm for $Y_t$ is chosen under a fixed reward mechanism. Under **reward distribution shift** or altered **information propagation**, a myopic choice may:
- under-exploit **transition edges** that create leverage on $Y_{t'}$,
- over-explore variables with **no lasting effect**,
- or assume **forced stationarity**, leading to suboptimal long-run reward.

---

## Algorithmic recipe (POMIS+ sequences)
**Input.** Time-expanded causal diagram; target horizon(s); candidate manipulative sets per slice.

**Procedure.**
1. For each $t$, construct the **temporal model** $\mathcal{M}_t$ and expose **transition edges** to slices $>t$.
2. Compute **IB+** around $t$ toward the target reward $Y_{t'}$.
3. Prune to **QIB** using temporal ancestors/mediators/confounders of $Y_{t'}$.
4. Choose a **POMIS+** set $\mathbf{X}_t \subseteq \mathrm{QIB}_t$.
5. Repeat across time steps to obtain a **POMIS+ intervention sequence**.
6. Execute the sequence; track **reward distribution shift** via observed $Y_t$ under $\operatorname{do}(\mathbf{X}_t)$.

This focuses exploration on variables whose effects **persist through time**.

---

## What we demonstrate
- A causal formalization of **NS-SCM-MAB** with precise **reward distribution shift** and **temporal model** semantics.  
- A **graphical characterization** for non-myopic intervention design using **IB+** and **QIB**.  
- A constructive method to build **POMIS+ intervention sequences** that exploit **information propagation**.  
- **Experiments** showing improved **cumulative regret** and higher **optimal-action probability** versus stationary or naive baselines under non-stationarity.

---

## Practical notes
- **Arms as interventions.** An arm is $\operatorname{do}(\mathbf{X}_t{=}\mathbf{x}_t)$; arms can be **sets** of variables.  
- **History-conditioned rewards.** We condition on $I_{1:t-1}$ to reflect how earlier interventions shape the current temporal model.  
- **Non-myopic selection.** Variables irrelevant to $Y_t$ may still belong to $\mathbf{X}_t$ if **transition edges** make them valuable for $Y_{t'}$.

---

## Paper/resources
- Paper PDF: [link](https://openreview.net/pdf?id=F4LhOqhxkk)
- Code: [link](https://github.com/yeahoon-k/NS-SCMMAB)

