---
layout: page
title: "Non-Stationary Structural Causal Bandits — Explanation"
permalink: /ns-scb-explainer/
---
{% include mathjax.html %}

# Non-Stationary Structural Causal Bandits

## Background
In a stationary environment, the agent leverages the causal diagram to identify intervention sets—excluding irrelevant variables—that maximize expected reward, and this choice remains valid over time (Lee & Bareinboim, 2018). This works when the environment is fixed.

<p align="center">
  <img src="{{ '/assets/img/s_and_ns1.png' | relative_url }}" alt="causal diagrams with transition edges" width="600">
</p>

**When the world drifts.** Under non-stationarity, the same behavior becomes myopic. The reward mechanism and the information that matters can change over time, so yesterday’s optimal arm can degrade today.

**What the agent is up against.** The hidden data-generating SCM evolves over time. We analyze it through a time-expanded causal diagram (i.e., temporal models) whose transition edges encode how information from earlier steps can affect later variables. This changing information flow produces reward disparities across time, which we view as a reward distribution shift given by the intervention history. The key question becomes: which past variables actually transmit useful information to future rewards?

**How the agent should act.** We reason in temporal models on a time-expanded causal diagram to separate what changes (reward distribution and effective information flow) from what remains stable. On this basis, we introduce POMIS$^+$—a time-aware extension of POMIS—together with two graphical concepts IB$^+$ and QIB to design non-myopic intervention sequences. Concretely, the agent may intervene on variables that are irrelevant to the current reward but crucial for a future reward via transition edges. This structure-aware strategy yields lower cumulative regret under reward distribution shifts.


## Problem setting (NS-SCM-MAB)
Let the bandit environment be generated by a time-indexed SCM (i.e., temporal model) $ \mathcal{M}_t=\langle \mathbf{U}_t,\mathbf{V}_t,\mathcal{F}_t, P(\mathbf{U}_t)\rangle,\ t=1,2,\dots $with reward variable $Y_t\in \mathbf{V}_t$. An arm corresponds to an intervention $ \operatorname{do}(\mathbf{X}_t{=}\mathbf{x}_t) $ on a manipulative set $ \mathbf{X}_t \subseteq \mathbf{V}_t $. Transition edges between time slices encode how variables at time $t$ transmit information to variables at $t{+}1$.


## Temporal model view
We reason with **temporal models** $ \mathcal{M}_t $ that respect cross-time dependencies in the **time-expanded causal diagram**. This clarifies:
- which variables at time $t$ are **parents** (direct or via transitions) of $Y_{t'}$ for $t'>t$,
- which **transition edges** carry useful information toward $Y_{t'}$,
- and how interventions at $t$ influence $Y_{t'}$ beyond a single slice.

This view separates **what changes** (reward distribution shift and effective information flow) from **what remains stable**, enabling **structure-aware** action selection.


## POMIS+ (temporal extension of POMIS)
**POMIS** identifies **Possibly-Optimal Minimal Intervention Sets** for a single slice. **POMIS$^+$** extends this idea to time by selecting a sequence
$(\mathbf{X}_{t_1},\ \mathbf{X}_{t_2},\ \dots,\ \mathbf{X}_{t_k})$
such that intervening on these sets at selected steps yields high expected future reward $ \mathbb{E}[Y_{t'}] $ for target times $ t'\ge t_k $.  
Importantly, $ \mathbf{X}_t $ may include variables **irrelevant to $Y_t$** but **crucial for $Y_{t'}$** via **transition edges**.


## IB$^+$ and QIB (graphical characterization)
To characterize non-myopic interventions on the time-expanded graph:

1. **IB$^+$ (Interventional Border for Subsequent Time Steps).**  
   A graphical frontier that gathers candidates in $ \mathbf{V}_t $ whose manipulation can still **alter information propagation** toward $Y_{t'}$.

2. **QIB (Qualified Interventional Border).**  
   A refinement of IB+ that filters out nodes whose manipulation would be redundant given the temporal model (e.g., blocked by **mediators** or **confounders**). QIB retains only candidates that can **change the distribution of ancestors of $Y_{t'}$** that matter.

**POMIS$^+$** is obtained by assembling **QIB-validated** subsets across time that maximize (or approximately maximize) the expected future reward under the **do-operator**.

## Why stationary solutions fail
In a **stationary SCM-MAB**, the optimal arm for $Y_t$ is chosen under a fixed reward mechanism. Under **reward distribution shift** or altered **information propagation**, a myopic choice may:
- under-exploit **transition edges** that create leverage on $Y_{t'}$,
- over-explore variables with **no lasting effect**,
- or assume **forced stationarity**, leading to suboptimal long-run reward.

---

## What we demonstrate
- A causal formalization of **NS-SCM-MAB** with precise **reward distribution shift** and **temporal model** semantics.  
- A **graphical characterization** for non-myopic intervention design using **IB+** and **QIB**.  
- A constructive method to build **POMIS+ intervention sequences** that exploit **information propagation**.  
- **Experiments** showing improved **cumulative regret** and higher **optimal-action probability** versus stationary or naive baselines under non-stationarity.


## Paper/resources
- Paper PDF: [link](https://openreview.net/pdf?id=F4LhOqhxkk)
- Code: [link](https://github.com/yeahoon-k/NS-SCMMAB)
