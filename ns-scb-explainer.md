---
layout: page
title: "Non-Stationary Structural Causal Bandits — Explanation"
permalink: /ns-scb-explainer/
---
{% include mathjax.html %}

# Non-Stationary Structural Causal Bandits
---

## Background
In a stationary world, the agent uses the causal diagram to pick a manipulative set and pull an arm that excludes irrelevant variables and targets the drivers of the reward. This works when the environment is fixed.

<p align="center">
  <object data="{{ '/assets/figs/s_and_ns.pdf#page=1&zoom=page-width' | relative_url }}"
          type="application/pdf"
          width="820" height="540">
    <a href="{{ '/assets/figs/ns-scb-overview.pdf' | relative_url }}">Open PDF</a>
  </object>
</p>


**When the world drifts.** Under non-stationarity, the same behavior becomes myopic. The reward mechanism and the information that matters can change over time, so yesterday’s optimal arm can degrade today.

**What the agent is up against.** The hidden data-generating process is better described by a time-indexed SCM (i.e., temporal models) with transition edges that carry information from one time step to the next. We view non-stationarity as a reward distribution shift conditioned on the intervention history. The key question becomes: which past variables actually transmit useful information to future rewards?

**How the agent should act.** We reason in temporal models on a time-expanded causal diagram to separate what changes (reward distribution and effective information flow) from what remains stable. On this basis, we introduce POMIS$^+$—a time-aware extension of POMIS—together with two graphical concepts IB$^+$ and QIB to design non-myopic intervention sequences. Concretely, the agent may intervene on variables that are irrelevant to the current reward but crucial for a future reward via transition edges. This structure-aware strategy yields lower cumulative regret under reward distribution shifts.


## Problem setting (NS-SCM-MAB)
Let the bandit environment be generated by a time-indexed SCM (i.e., temporal model) $ \mathcal{M}_t=\langle \mathbf{U}_t,\mathbf{V}_t,\mathcal{F}_t, P(\mathbf{U}_t)\rangle,\ t=1,2,\dots $with reward variable $Y_t\in \mathbf{V}_t$. An **arm** corresponds to an intervention $ \operatorname{do}(\mathbf{X}_t{=}\mathbf{x}_t) $ on a **manipulative set** $ \mathbf{X}_t \subseteq \mathbf{V}_t $. We use **transition edges** between slices to encode how variables at time $t$ transmit information to variables at $t{+}1$.

A **reward distribution shift** occurs when the conditional reward distribution changes across time given the intervention history $I_{1:t-1}$:
$$
P\!\big(Y_t \mid \operatorname{do}(\mathbf{X}_t{=}\mathbf{x}_t), I_{1:t-1}\big)
\;\neq\;
P\!\big(Y_{t'} \mid \operatorname{do}(\mathbf{X}_{t'}{=}\mathbf{x}_{t'}), I_{1:t'-1}\big)
\quad \text{for some } t\neq t'.
$$
Stationary SCM-MAB solutions are **myopic** here because they optimize $Y_t$ in isolation and ignore cross-time information propagation.


## Temporal model view
We reason with **temporal models** $ \mathcal{M}_t $ that respect cross-time dependencies in the **time-expanded causal diagram**. This clarifies:
- which variables at time $t$ are **parents** (direct or via transitions) of $Y_{t'}$ for $t'>t$,
- which **transition edges** carry useful information toward $Y_{t'}$,
- and how interventions at $t$ influence $Y_{t'}$ beyond a single slice.

This view separates **what changes** (reward distribution shift and effective information flow) from **what remains stable**, enabling **structure-aware** action selection.

---

## POMIS+ (temporal extension of POMIS)
**POMIS** identifies **Possibly-Optimal Manipulative Sets** for a single slice. **POMIS+** extends this idea to time by selecting a sequence
$$
(\mathbf{X}_{t_1},\ \mathbf{X}_{t_2},\ \dots,\ \mathbf{X}_{t_k})
$$
such that intervening on these sets at selected steps yields high expected future reward $ \mathbb{E}[Y_{t'}] $ for target times $ t'\ge t_k $.  
Importantly, $ \mathbf{X}_t $ may include variables **irrelevant to $Y_t$** but **crucial for $Y_{t'}$** via **transition edges**.

---

## IB+ and QIB (graphical characterization)
To characterize non-myopic interventions on the time-expanded graph:

1. **IB+ (Interventional Border for Subsequent Time Steps).**  
   A graphical frontier that gathers candidates in $ \mathbf{V}_t $ whose manipulation can still **alter information propagation** toward $Y_{t'}$.

2. **QIB (Qualified Interventional Border).**  
   A refinement of IB+ that filters out nodes whose manipulation would be redundant given the temporal model (e.g., blocked by **mediators** or **confounders**). QIB retains only candidates that can **change the distribution of ancestors of $Y_{t'}$** that matter.

**POMIS+** is obtained by assembling **QIB-validated** subsets across time that maximize (or approximately maximize) the expected future reward under the **do-operator**.

---

## Why stationary solutions fail
In a **stationary SCM-MAB**, the optimal arm for $Y_t$ is chosen under a fixed reward mechanism. Under **reward distribution shift** or altered **information propagation**, a myopic choice may:
- under-exploit **transition edges** that create leverage on $Y_{t'}$,
- over-explore variables with **no lasting effect**,
- or assume **forced stationarity**, leading to suboptimal long-run reward.

---

## Algorithmic recipe (POMIS+ sequences)
**Input.** Time-expanded causal diagram; target horizon(s); candidate manipulative sets per slice.

**Procedure.**
1. For each $t$, construct the **temporal model** $ \mathcal{M}_t $ and expose **transition edges** to slices $>t$.
2. Compute **IB+** around $t$ toward the target reward $Y_{t'}$.
3. Prune to **QIB** using temporal ancestors/mediators/confounders of $Y_{t'}$.
4. Choose a **POMIS+** set $ \mathbf{X}_t \subseteq \mathrm{QIB}_t $.
5. Repeat across time steps to obtain a **POMIS+ intervention sequence**.
6. Execute the sequence; track **reward distribution shift** via observed $Y_t$ under $ \operatorname{do}(\mathbf{X}_t) $.

This focuses exploration on variables whose effects **persist through time**.

---

## What we demonstrate
- A causal formalization of **NS-SCM-MAB** with precise **reward distribution shift** and **temporal model** semantics.  
- A **graphical characterization** for non-myopic intervention design using **IB+** and **QIB**.  
- A constructive method to build **POMIS+ intervention sequences** that exploit **information propagation**.  
- **Experiments** showing improved **cumulative regret** and higher **optimal-action probability** versus stationary or naive baselines under non-stationarity.

---

## Paper/resources
- Paper PDF: [link](https://openreview.net/pdf?id=F4LhOqhxkk)
- Code: [link](https://github.com/yeahoon-k/NS-SCMMAB)
