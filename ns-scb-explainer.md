---
layout: page
title: "Non-Stationary Structural Causal Bandits — Explanation"
permalink: /ns-scb-explainer/
---
{% include mathjax.html %}

# Non-Stationary Structural Causal Bandits

## Background
In a stationary environment, the agent can leverage the causal diagram to identify optimal intervention sets—excluding irrelevant variables—that maximize expected reward, and this choice remains valid over time (Lee & Bareinboim, 2018). This works when the environment is fixed for all time steps, as illustrated on the left side of fig. 1.

<p align="center">
  <img src="{{ '/assets/img/s_and_ns1.png' | relative_url }}" alt="causal diagrams with transition edges" width="600">
  <em>Figure 1</em>
</p>

**When the world drifts.** Under non-stationarity, the same behavior becomes myopic. The reward mechanism and the information that matters can change over time, so _yesterday’s optimal arm can degrade today_.

**What the agent is up against.** The hidden data-generating mechanism evolves over time. We analyze it through a time-expanded causal model (i.e., temporal models) whose transition edges encode how information from earlier steps can affect later variables. This changing information flow produces reward disparities across time, which we view as a reward distribution shift given by the intervention history. The key question becomes: _which past variables actually transmit useful information to future rewards?_

**How the agent should act.** We reason on a time-expanded causal diagram to separate what changes (reward distribution and effective information flow) from what remains stable. On this basis, we introduce POMIS$^+$—a time-aware extension of POMIS—together with two graphical concepts IB$^+$ and QIB to design non-myopic intervention sequences. Concretely, the agent may intervene on variables that are _irrelevant to the current reward but crucial for a future reward_ via transition edges. This structure-aware strategy clarifies which intervention sets to choose at each time step under reward distribution shifts, effectively identifying the **intervention sequence**.

## Problem setting (NS-SCM-MAB)
Let the bandit environment be generated by a time-indexed SCM (i.e., temporal model) $ \mathcal{M}\_t=\langle \mathbf{U}\_t,\mathbf{V}\_t,\mathcal{F}\_t, P(\mathbf{U}\_t)\rangle,\ t=1,2,\dots $with reward variable $Y\_t\in \mathbf{V}\_t$. An arm corresponds to an intervention $ \operatorname{do}(\mathbf{X}\_t{=}\mathbf{x}\_t) $ on a manipulative set $ \mathbf{X}\_t \subseteq \mathbf{V}\_t $. Transition edges between time slices encode how variables at time $t$ transmit information to variables at $t{+}1$ (right panel in fig. 1).

A reward distribution shift occurs when the conditional reward distribution changes across time given the intervention history $I_{1:t-1}$:
\\[
P(Y\_t \mid \operatorname{do}(\mathbf{X}\_t{=}\mathbf{x}\_t), I\_{1:t-1}) \neq P(Y\_{t'} \mid \operatorname{do}(\mathbf{X}\_{t'}{=}\mathbf{x}\_{t'}), I\_{1:t'-1})
\\]
Stationary SCM-MAB solutions are **myopic** here because they optimize $Y\_t$ in isolation and ignore cross-time information propagation.

### Example
Under the time-slice graph \( \mathcal{G}[V\_1] \), the agent first computes the **MUCT** around the current reward \(Y\_1\).  
Because both \(X\_1\) and \(Z\_1\) lie on upstream paths that reach \(Y\_1\), the minimal upstream carrier at \(t{=}1\) is \( \{X\_1, Z\_1, Y\_1\} \).  
Given this MUCT, the **in-slice interventional border (IB)** for \(Y\_1\) is **empty**—there is no strictly smaller parental frontier that must be manipulated at this slice. Formally:
\\[
\operatorname{MUCT}(\mathcal{G}[V\_1],\, Y\_1)=\{X\_1,\, Z\_1,\, Y\_1\},
\qquad
\operatorname{IB}(\mathcal{G}[V\_1],\, Y\_1)=\varnothing.
\\]

Next, by **Proposition 4** in the paper, we examine the **predetermined** (intervened) graph  
\( \mathcal{G}[V\_1]\_{\bar{X}\_1} \), i.e., we fix \(X\_1\) and recompute the local objects.  
Once \(X\_1\) is fixed, the only variable that still forms the interventional frontier toward \(Y\_1\) is \(Z\_1\); the MUCT correspondingly collapses to \(\{Y\_1\}\). Thus:
\\[
\operatorname{MUCT}(\mathcal{G}[V\_1]\_{\overline{X}\_1},\, Y\_1)=\{Y\_1\},
\qquad
\operatorname{IB}(\mathcal{G}[V\_1]\_{\overline{X}\_1},\, Y\_1)=\{Z\_1\}.
\\]

**Conclusion (POMIS at \(t{=}1\)).**  
Putting these together, the admissible manipulative sets for \(Y\_1\) at time \(t{=}1\) are the **empty set** (no intervention needed at the current slice) and the **singleton \(\{Z\_1\}\)** that becomes active once \(X\_1\) is predetermined. Therefore,
\\[
\mathrm{POMIS}\_1(Y\_1)=\{\;\varnothing,\ \{Z\_1\}\;\}.
\\]

*Intuition.* Without predetermining \(X\_1\), there is no compulsory parent to manipulate at slice 1 (IB is empty). After fixing \(X\_1\), \(Z\_1\) is the unique handle that still reaches \(Y\_1\), so \(\{Z\_1\}\) joins \(\varnothing\) as a valid choice in \( \mathrm{POMIS}\_1 \).
<p align="center">
  <img src="{{ '/assets/img/example1.png' | relative_url }}" alt="causal diagrams with transition edges" width="600">
  <em>Figure 2</em>
</p>


## POMIS+ (temporal extension of POMIS)
**POMIS** identifies **Possibly-Optimal Minimal Intervention Sets** for a single slice. POMIS$^+$ extends this idea to time by selecting a sequence
$(\mathbf{X}\_{t\_1},\ \mathbf{X}\_{t_2},\ \dots,\ \mathbf{X}\_{t\_k})$
such that intervening on these sets at selected steps yields high expected future reward $ \mathbb{E}[Y\_{t'}] $ for target times $ t'\ge t\_k $.  
Importantly, $ \mathbf{X}\_t $ may include variables irrelevant to $Y\_t$ but crucial for $Y\_{t'}$ via transition edges.


## IB$^+$ and QIB (graphical characterization)
To characterize non-myopic interventions on the time-expanded graph:

1. **IB$^+$ (Interventional Border for Subsequent Time Steps).**  
   A graphical frontier that gathers candidates in $ \mathbf{V}\_t $ whose manipulation can still **alter information propagation** toward $Y\_{t'}$.

2. **QIB (Qualified Interventional Border).**  
   A refinement of IB$^+$ that filters out nodes whose manipulation would be redundant given the temporal model (e.g., blocked by mediators or confounders). QIB retains only candidates that can change the distribution of ancestors of $Y\_{t'}$ that matter.

The **POMIS$^+$ for $(t,t')$** is obtained by the **union**
\\[
\mathrm{POMIS}^{+}\_{t,t'} \;=\; \mathrm{IB}^{+}\_{t,t'}\big(G[\!\bigcup\_{i=t}^{t'} V\_i], Y\_{t'}\big)\; \cup\; \mathrm{QIB}\_t\big(G[V\_t], Y\_t\big).
\\]
Under the time-slice Markov assumption, it suffices to take $t'=t+1$ (i.e., look only one step ahead).

## Why stationary solutions fail
In a **stationary SCM-MAB**, the optimal arm for $Y_t$ is chosen under a fixed reward mechanism. Under **reward distribution shift** or altered **information propagation**, a myopic choice may:
- under-exploit **transition edges** that create leverage on $Y_{t'}$,
- over-explore variables with **no lasting effect**,
- or assume **forced stationarity**, leading to suboptimal long-run reward.

---

## What we demonstrate
- A causal formalization of **NS-SCM-MAB** with precise **reward distribution shift** and **temporal model** semantics.  
- A **graphical characterization** for non-myopic intervention design using **IB+** and **QIB**.  
- A constructive method to build **POMIS+ intervention sequences** that exploit **information propagation**.  
- **Experiments** showing improved **cumulative regret** and higher **optimal-action probability** versus stationary or naive baselines under non-stationarity.


## Paper/resources
- Paper PDF: [link](https://openreview.net/pdf?id=F4LhOqhxkk)
- Code: [link](https://github.com/yeahoon-k/NS-SCMMAB)
